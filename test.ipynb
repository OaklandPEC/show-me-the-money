{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a CSV file from Netfile results with required fields for Socrata app\n",
    "\n",
    "Socrata required fields:\n",
    "[\n",
    "  \"filer_id\",\n",
    "  \"filer_name\",\n",
    "  \"receipt_date\"\n",
    "  \"amount\",\n",
    "  \"contributor_name\",\n",
    "  \"contributor_address\",\n",
    "  \"contributor_location\",\n",
    "  \"contributor_type\",\n",
    "  \"election_year\",\n",
    "  \"jurisdiction\",\n",
    "  \"office\",\n",
    "  \"party\",\n",
    "]\n",
    "\"\"\"\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "import requests\n",
    "from .query_v2_api import get_filer, get_auth_from_env_file\n",
    "#\n",
    "import datetime as dt\n",
    "from date_range import filing_date, is_iso_str_in_range\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "EXAMPLE_DATA_DIR = 'example'\n",
    "INPUT_DATA_DIR = 'input'\n",
    "OUTPUT_DATA_DIR = 'output'\n",
    "FILER_TO_CAND_PATH = f'{INPUT_DATA_DIR}/filer_to_candidate.csv'\n",
    "SOCRATA_EXPEND_SCHEMA_PATH = f'{INPUT_DATA_DIR}/socrata_schema_expend_fields.json'\n",
    "\n",
    "CONTRIBUTION_FORMS = [ 'F460A', 'F460C' ]\n",
    "LATE_CONTRIBUTION_FORM_PATTERN = 'F497'\n",
    "EXPENDITURE_FORM = 'F460E'\n",
    "BASE_URL = 'https://netfile.com/api/campaign'\n",
    "PARAMS = { 'aid': 'COAK' }\n",
    "TIMEOUT = 7\n",
    "SKIP_LIST = [\n",
    "    '95096360-1f8d-4502-a70b-451dc6a9a0b3',\n",
    "    '8deaa063-883b-4459-a32a-558653ca4fef',\n",
    "    '04855230-5387-4cd9-9cfa-3e0c10fe5318'\n",
    "]\n",
    "OAKLAND_MISSPELLINGS = [\n",
    "    'OAKLAND',\n",
    "    'OakLand',\n",
    "    'Oaklalnd',\n",
    "    'Oaklannd',\n",
    "    'Okaland',\n",
    "    'oakland'\n",
    "]\n",
    "AUTH=get_auth_from_env_file()\n",
    "\n",
    "class TimeoutAdapter(requests.adapters.HTTPAdapter):\n",
    "    \"\"\" Will this allow me to retry on timeout? \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = kwargs.pop('timeout', TIMEOUT)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, *args, **kwargs):\n",
    "        kwargs['timeout'] = kwargs.get('timeout', self.timeout)\n",
    "        return super().send(request, *args, **kwargs)\n",
    "\n",
    "session = requests.Session()\n",
    "session.hooks['response'] = [ lambda response, *args, **kwargs: response.raise_for_status() ]\n",
    "retry_strategy = requests.adapters.Retry(total=5, backoff_factor=2)\n",
    "adapter = TimeoutAdapter(max_retries=retry_strategy)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "def select_response_meta(response_body):\n",
    "    \"\"\" Get props needed from response body for further requests \"\"\"\n",
    "    print(response_body['offset'], response_body['limit'])\n",
    "    return {\n",
    "        'page_number': response_body['pageNumber'],\n",
    "        'has_next_page': response_body['hasNextPage'],\n",
    "        'total': response_body['totalCount'],\n",
    "        'count': response_body['count'],\n",
    "        'limit': response_body['limit'],\n",
    "        'offset': response_body['offset'],\n",
    "        'next_offset': response_body['limit'] + response_body['offset'] if response_body['hasNextPage'] else None\n",
    "    }\n",
    "\n",
    "def get_filings(offset=0) -> tuple[pd.DataFrame, list[dict], dict]:\n",
    "    \"\"\" Get a page of filings\n",
    "        Return fields required by Socrata\n",
    "    \"\"\"\n",
    "    params = { **PARAMS }\n",
    "\n",
    "    if offset > 0:\n",
    "        params['offset'] = offset\n",
    "\n",
    "    res = session.get(f'{BASE_URL}/filing/v101/filings', params=params, auth=AUTH)\n",
    "    body = res.json()\n",
    "\n",
    "    return body['results'], select_response_meta(body)\n",
    "\n",
    "def get_all_filings() -> list[dict]:\n",
    "    \"\"\" Fetch all filings \"\"\"\n",
    "    filings, response_meta = get_filings()\n",
    "    print(response_meta['total'])\n",
    "\n",
    "    next_offset = response_meta['next_offset']\n",
    "    end = ''\n",
    "    while next_offset is not None:\n",
    "        results, meta = get_filings(offset=next_offset)\n",
    "        next_offset = meta['next_offset']\n",
    "        filings += results\n",
    "        print('¡', end=end, flush=True)\n",
    "    print('')\n",
    "\n",
    "    return filings\n",
    "\n",
    "def get_trans() -> list[dict]:\n",
    "    \"\"\" Fetch all transactions \"\"\"\n",
    "    params = {\n",
    "        **PARAMS,\n",
    "        'parts': 'All',\n",
    "        'limit': 1000\n",
    "    }\n",
    "    offset = 0\n",
    "    has_next_page = True\n",
    "\n",
    "    results = []\n",
    "    while has_next_page is True:\n",
    "        if offset > 0:\n",
    "            params['offset'] = offset\n",
    "\n",
    "        try:\n",
    "            res = session.get(f'{BASE_URL}/cal/v101/transaction-elements', params=params, auth=AUTH)\n",
    "        except requests.HTTPError as exc:\n",
    "            print(f'{exc.response.status_code} for request {exc.response.url}')\n",
    "            print(exc.response.json())\n",
    "            params_no_parts = { ** params }\n",
    "            params_no_parts.pop('parts')\n",
    "            res = session.get(f'{BASE_URL}/cal/v101/transaction-elements', params=params_no_parts, auth=AUTH)\n",
    "\n",
    "        body = res.json()\n",
    "        results = results + body['results']\n",
    "\n",
    "        has_next_page = body['hasNextPage']\n",
    "        offset = offset + body['limit']\n",
    "        print('\\u258a', end='', flush=True)\n",
    "\n",
    "    print('')\n",
    "    return [tran for tran in results if is_iso_str_in_range(tran['tran_Date'], filing_date.get(f\"{tran['filingId']}\", set())) or tran['rec_Type']=='S497']\n",
    "\n",
    "def get_trans_for_filing(filing_nid, offset=0) -> tuple[list[dict], dict]:\n",
    "    \"\"\" Get a page of transactions\n",
    "        for a filingNid\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        **PARAMS,\n",
    "        'filingNid': filing_nid,\n",
    "        'parts': 'All'\n",
    "    }\n",
    "\n",
    "    if offset > 0:\n",
    "        params['offset'] = offset\n",
    "\n",
    "    res = session.get(f'{BASE_URL}/cal/v101/transaction-elements', params=params, auth=AUTH)\n",
    "    body = res.json()\n",
    "\n",
    "    return body['results'], select_response_meta(body)\n",
    "\n",
    "def get_all_trans_for_filing(filing_nid):\n",
    "    \"\"\" Get all transactions for a single filing_nid \"\"\"\n",
    "    next_offset = 0\n",
    "    params = {\n",
    "        'filing_nid': filing_nid\n",
    "    }\n",
    "\n",
    "    transactions, meta = get_trans_for_filing(**params)\n",
    "    end = '/' if meta['total'] > meta['limit'] else ' '\n",
    "    if end != ' ':\n",
    "        print('')\n",
    "    print(meta['total'], end=end, flush=True)\n",
    "\n",
    "    next_offset = meta.get('next_offset')\n",
    "    while next_offset is not None:\n",
    "        results, meta = get_trans_for_filing(**params, offset=next_offset)\n",
    "        next_offset = meta.get('next_offset')\n",
    "        prog_char = '¡' if len(results) > 0 else '.'\n",
    "        transactions += results\n",
    "        print(next_offset, end='', flush=True)\n",
    "\n",
    "    return transactions\n",
    "\n",
    "def get_trans_for_filings(filing_nids: set) -> list[dict]:\n",
    "    \"\"\" Get all transactions for set of filing netfile IDs \"\"\"\n",
    "    transactions = []\n",
    "    for filing_nid in filing_nids:\n",
    "        if filing_nid in SKIP_LIST:\n",
    "            continue\n",
    "        transactions += [tran for tran in get_all_trans_for_filing(filing_nid) if is_iso_str_in_range(tran['tran_Date'], filing_date.get(f\"{tran['filingId']}\", set())) or tran['rec_Type']=='S497']\n",
    "    print('')\n",
    "\n",
    "    return transactions\n",
    "\n",
    "def get_all_filers(filer_nids: set) -> list[dict]:\n",
    "    \"\"\" Fetch all filers \"\"\"\n",
    "    filers = []\n",
    "    for filer_nid in filer_nids:\n",
    "        filers += get_filer(filer_nid)\n",
    "        print('¡', end='', flush=True)\n",
    "    print('')\n",
    "\n",
    "    return filers\n",
    "\n",
    "def fetch_source_data() -> tuple[list[dict]]:\n",
    "    print('===== Get filings =====')\n",
    "    filings = get_all_filings()\n",
    "\n",
    "    print('===== Get transactions =====')\n",
    "    transactions = get_trans()\n",
    "\n",
    "    print('===== Get filers =====')\n",
    "    unique_filer_nids = set(f['filerMeta']['filerId'] for f in filings)\n",
    "    filers = get_all_filers(unique_filer_nids)\n",
    "\n",
    "    return filings, transactions, filers\n",
    "\n",
    "def load_source_data() -> tuple[list[dict]]:\n",
    "    source_data = []\n",
    "    for f in ['filings', 'transactions', 'filers']:\n",
    "        source_data.append(json.loads(Path(f'example/{f}.json').read_text(encoding='utf8')))\n",
    "\n",
    "    return tuple(source_data)\n",
    "\n",
    "def get_source_data(download=False) -> tuple[list[dict]]:\n",
    "    if download:\n",
    "        filings, transactions, filers = fetch_source_data()\n",
    "\n",
    "        save_source_data({\n",
    "            'filings': filings,\n",
    "            'transactions': transactions,\n",
    "            'filers': filers\n",
    "        })\n",
    "\n",
    "        return filings, transactions, filers\n",
    "    else:\n",
    "        return load_source_data()\n",
    "\n",
    "def df_from_filings(filings):\n",
    "    \"\"\" Transform filings into Pandas DataFrame \"\"\"\n",
    "    return pd.DataFrame([{\n",
    "        'filer_nid': f['filerMeta']['filerId'],\n",
    "        'filing_nid': f['filingNid'],\n",
    "        'filing_date': f['calculatedDate'],\n",
    "        'form': f['specificationRef']['name'].replace('FPPC', ''),\n",
    "        'committee_name': f['filerMeta']['commonName']\n",
    "    } for f in filings ])\n",
    "\n",
    "def get_relative_location(city: str, state: str) -> str:\n",
    "    \"\"\" Get location relative to Oakland, CA\n",
    "        from address city & state\n",
    "    \"\"\"\n",
    "    if city == 'Oakland':\n",
    "        return 'In Oakland'\n",
    "    if state == 'CA':\n",
    "        return 'Other CA City'\n",
    "    return 'Out of State'\n",
    "\n",
    "def get_address(addresses: list[dict]) -> dict[str, str]:\n",
    "    \"\"\" Get street address from addresses, or return empty string\n",
    "        Expects address dicts in the form\n",
    "        {\n",
    "            line1\n",
    "            line2\n",
    "            city\n",
    "            state\n",
    "            zip\n",
    "        }\n",
    "    \"\"\"\n",
    "    keys = [\n",
    "        'contributor_address',\n",
    "        'city',\n",
    "        'state',\n",
    "        'zip_code',\n",
    "        'contributor_region'\n",
    "    ]\n",
    "\n",
    "    if len(addresses) < 1:\n",
    "        return { k: v for k, v in zip_longest(keys, '', fillvalue='') }\n",
    "\n",
    "    address = addresses[0]\n",
    "\n",
    "    street = f'{address.get(\"line1\") or \"\"} {address.get(\"line2\") or \"\"}'.strip()\n",
    "    city = 'Oakland' if address.get('city') in OAKLAND_MISSPELLINGS else address.get('city')\n",
    "    city_state_zip = ' '.join([city or '', address['state'] or '', address['zip'] or '']).strip()\n",
    "    contributor_address = f'{street}, {city_state_zip}' if (street and city_state_zip) else ''\n",
    "\n",
    "    return {\n",
    "        k: v\n",
    "        for k, v\n",
    "        in zip(keys, [\n",
    "            contributor_address,\n",
    "            city,\n",
    "            address['state'],\n",
    "            address['zip'],\n",
    "            get_relative_location(city, address['state'])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "def get_location(addresses):\n",
    "    \"\"\" Get (long, lat) from addresses, or return empty string \"\"\"\n",
    "    if len(addresses) <= 0:\n",
    "        return ''\n",
    "\n",
    "    address = addresses[0]\n",
    "    long = address['longitude']\n",
    "    lat = address['latitude']\n",
    "\n",
    "    if long is None or lat is None:\n",
    "        return ''\n",
    "\n",
    "    # long_range = (0.2275, 0.455) # approx. b/w .25 mi and .5 mi @ 38ºN\n",
    "    # lat_range = (0.2173, 0.575) # approx. b/w .25 mi and .5 mi @ 38ºN\n",
    "    long_range = 0,0\n",
    "    lat_range = 0,0\n",
    "    adjusted = [str(float(long) + uniform(*long_range)), str(float(lat) + uniform(*lat_range))]\n",
    "    return f'POINT ({\" \".join(adjusted)})'\n",
    "\n",
    "def get_contrib_category(entity_code):\n",
    "    \"\"\" Translate three-letter entityCd into human readable entity code \"\"\"\n",
    "    return {\n",
    "        'RCP': 'Committee',\n",
    "        'IND': 'Individual',\n",
    "        'OTH': 'Business/Other',\n",
    "        'COM': 'Committee',\n",
    "        'PTY': 'Political Party',\n",
    "        'SCC': 'Small Contributor Committee'\n",
    "    }.get(entity_code)\n",
    "\n",
    "def df_from_trans(transactions):\n",
    "    \"\"\" Transform transaction dict into Pandas DataFrame \"\"\"\n",
    "    tran_cols = [\n",
    "        'tran_id',\n",
    "        'filing_nid',\n",
    "        'contributor_name',\n",
    "        'contributor_type',\n",
    "        'contributor_category',\n",
    "        'contributor_address',\n",
    "        'city',\n",
    "        'state',\n",
    "        'zip_code',\n",
    "        'contributor_region',\n",
    "        'contributor_location',\n",
    "        'amount',\n",
    "        'receipt_date',\n",
    "        'expn_code',\n",
    "        'expenditure_description',\n",
    "        'form',\n",
    "        'party'\n",
    "    ]\n",
    "\n",
    "    transaction_data = [\n",
    "        {\n",
    "            'tran_id': t['transaction']['tranId'],\n",
    "            'filing_nid': t['filingNid'],\n",
    "            'contributor_name': t['allNames'],\n",
    "            'contributor_type': 'Individual' if t['transaction']['entityCd'] == 'IND' else 'Organization',\n",
    "            'contributor_category': get_contrib_category(t['transaction']['entityCd']),\n",
    "            **get_address(t['addresses']),\n",
    "            'contributor_location': None,\n",
    "            'amount': t['calculatedAmount'],\n",
    "            'receipt_date': t['transaction']['tranDate'],\n",
    "            'expn_code': t['transaction']['tranCode'],\n",
    "            'expenditure_description': t['transaction']['tranDscr'] or '',\n",
    "            'form': t['calTransactionType'],\n",
    "            'party': None,\n",
    "        } for t in transactions\n",
    "        if t.get('transaction') is not None # Skip incomplete transactions\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(transaction_data, columns=tran_cols)\n",
    "    df['receipt_date'] = pd.to_datetime(df['receipt_date'])\n",
    "    return df\n",
    "\n",
    "def df_from_filers(filers):\n",
    "    \"\"\" Transform filers into Pandas DataFrame \"\"\"\n",
    "    # filter out committees without CA SOS IDs\n",
    "    # as we have no way to join them to filings\n",
    "    return pd.DataFrame([ {\n",
    "        'filer_nid': f['filerNid'],\n",
    "        'filer_id': f['registrations'].get('CA SOS')\n",
    "    } for f in filers if f['registrations'].get('CA SOS') is not None\n",
    "    ]).astype({ 'filer_id': 'string' })\n",
    "\n",
    "def df_from_candidates() -> pd.DataFrame:\n",
    "    \"\"\" Get DataFrame of candidates from CSV \"\"\"\n",
    "    filer_to_cand_cols = [\n",
    "        'local_agency_id',\n",
    "        'filer_id',\n",
    "        'election_year',\n",
    "        'filer_name',\n",
    "        'filer_name_local',\n",
    "        'jurisdiction',\n",
    "        'office',\n",
    "        'start_date',\n",
    "        'end_date'\n",
    "    ]\n",
    "    filer_to_cand = pd.read_csv(FILER_TO_CAND_PATH, dtype={\n",
    "        'filer_name': 'string',\n",
    "        'is_terminated': 'string',\n",
    "        'sos_id': 'string',\n",
    "        'type': 'string',\n",
    "        'local_agency_id': 'string',\n",
    "        'election_year': int,\n",
    "        'candidate': 'string',\n",
    "        'contest': 'string',\n",
    "        'citywide': 'string',\n",
    "        'incumbent': 'string',\n",
    "        'start': 'string',\n",
    "        'end': 'string',\n",
    "        'is_winner': 'string',\n",
    "        'ballot_status': 'string'\n",
    "    })\n",
    "    filer_to_cand = filer_to_cand.rename(columns={\n",
    "        'sos_id': 'filer_id',\n",
    "        'filer_name': 'filer_name_local',\n",
    "        'type': 'jurisdiction',\n",
    "        'contest': 'office',\n",
    "        'candidate': 'filer_name',\n",
    "        'start': 'start_date',\n",
    "        'end': 'end_date'\n",
    "    }, errors='raise')[\n",
    "        filer_to_cand_cols\n",
    "    ].astype({ 'filer_id': 'string' })\n",
    "\n",
    "    filer_to_cand['end_date'] = pd.to_datetime(filer_to_cand['end_date'])\n",
    "    filer_to_cand['start_date'] = pd.to_datetime(filer_to_cand['start_date'])\n",
    "\n",
    "    return filer_to_cand\n",
    "\n",
    "def get_jurisdiction(row):\n",
    "    \"\"\" Get jurisdiction of office, one of\n",
    "        - Council District\n",
    "        - OUSD District\n",
    "        - Citywide\n",
    "    \"\"\"\n",
    "    if row['office'].lower().startswith('city council district '):\n",
    "        return 'Council District'\n",
    "    if row['office'].lower().startswith('ousd district'):\n",
    "        return 'Oakland Unified School District'\n",
    "    \n",
    "    return 'Citywide'\n",
    "\n",
    "def get_filing_deadlines():\n",
    "    \"\"\" Get filing deadlines from csv \"\"\"\n",
    "    date_fields = [ 'election_date', 'report_period_start', 'report_period_end', 'filing_deadline' ]\n",
    "    return pd.read_csv(f'{INPUT_DATA_DIR}/filing_deadlines.csv', parse_dates=date_fields)\n",
    "\n",
    "def merge_filings_and_trans(filings: pd.DataFrame, trans: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Return filings DataFrame joined with transactions DataFrame, dropping common columns \"\"\"\n",
    "    return filings.rename(columns={'form': 'filing_form'}).merge(trans, how='left', on='filing_nid')\n",
    "\n",
    "def save_source_data(json_data: list[dict]) -> None:\n",
    "    \"\"\" Save JSON data output from NetFile API \"\"\"\n",
    "    example_data_dir=Path(EXAMPLE_DATA_DIR)\n",
    "    example_data_dir.mkdir(exist_ok=True)\n",
    "    for endpoint_name, data in json_data.items():\n",
    "        (example_data_dir / f'{endpoint_name}.json').write_text(\n",
    "            json.dumps(data, indent=4\n",
    "        ), encoding='utf8')\n",
    "\n",
    "def save_previous_version(path_name):\n",
    "    \"\"\" Move existing file to `prev_${filename}` location \"\"\"\n",
    "    p = Path(path_name).resolve()\n",
    "    if p.exists():\n",
    "        new_file_name = 'prev_' + p.name\n",
    "        new_file_path = p.parent / new_file_name\n",
    "        p.rename(new_file_path)\n",
    "\n",
    "def main(filings, transactions, filers):\n",
    "    \"\"\" Query Netfile results 1 page at a time\n",
    "        Build Pandas DataFrame\n",
    "        and then save it as CSV\n",
    "\n",
    "        0. Get all elections, collect dates into ordered list\n",
    "        1. Query filing\n",
    "        2. For each filing, query transaction-elements?filingNid={filingNid}&parts=All\n",
    "        3. Match filingDate to electionDate, extract year from date\n",
    "        4. Query /filer/v101/filers/{filer_nid}, get electionInfluences[electionDate].seat.officeName\n",
    "    \"\"\"\n",
    "    filing_df = df_from_filings(filings)\n",
    "    filing_df['filing_date'] = pd.to_datetime(filing_df['filing_date'])\n",
    "\n",
    "    tran_df = df_from_trans(transactions)\n",
    "    filer_df = df_from_filers(filers)\n",
    "\n",
    "    expn_codes = pd.read_csv(f'{INPUT_DATA_DIR}/expenditure_codes.csv').rename(columns={\n",
    "        'description': 'expenditure_type'\n",
    "    })\n",
    "    tran_df = tran_df.merge(expn_codes, how='left', on='expn_code')\n",
    "\n",
    "    filer_to_cand = df_from_candidates()\n",
    "    filer_id_mapping = filer_to_cand.merge(filer_df, how='left', on='filer_id')\n",
    "    filer_filings = filer_id_mapping.merge(filing_df, how='left', on='filer_nid')\n",
    "    filing_trans = filer_filings.rename(columns={\n",
    "        'form': 'filing_form'\n",
    "    }).merge(tran_df, how='left', on='filing_nid')\n",
    "\n",
    "    df = filing_trans.astype({\n",
    "        'filer_name': 'string',\n",
    "        'contributor_name': 'string',\n",
    "        'contributor_type': 'string',\n",
    "        'contributor_address': 'string',\n",
    "        'amount': float\n",
    "    }).rename(columns={\n",
    "        'filing_nid': 'filing_id'\n",
    "    })\n",
    "    df['filer_name'] = df.apply(\n",
    "        lambda x: (\n",
    "            x['filer_name']\n",
    "            if x['jurisdiction'] == 'Candidate or Officeholder'\n",
    "            else x['filer_name_local']\n",
    "        ).strip(),\n",
    "        axis=1,\n",
    "        result_type='reduce'\n",
    "    )\n",
    "\n",
    "    df.to_csv(f'{EXAMPLE_DATA_DIR}/all_trans.csv', index=False)\n",
    "\n",
    "    common_cols = [ 'city', 'state', 'zip_code', 'committee_name', 'filing_id', 'tran_id' ]\n",
    "    contrib_cols = [\n",
    "        'tran_id',\n",
    "        'filing_id',\n",
    "        'filer_id',\n",
    "        'filer_name',\n",
    "        'committee_name',\n",
    "        'contributor_name',\n",
    "        'contributor_type',\n",
    "        'contributor_category',\n",
    "        'contributor_address',\n",
    "        'contributor_location',\n",
    "        'contributor_region',\n",
    "        'city',\n",
    "        'state',\n",
    "        'zip_code',\n",
    "        'amount',\n",
    "        'receipt_date',\n",
    "        'election_year',\n",
    "        'office',\n",
    "        'jurisdiction',\n",
    "        'party'\n",
    "    ]\n",
    "    contribs = df[df['form'].isin(CONTRIBUTION_FORMS)]\n",
    "    late_contribs = df[df['filing_form'] == '497']\n",
    "\n",
    "    filing_deadlines = get_filing_deadlines()\n",
    "    today = datetime(*datetime.now().timetuple()[:3])\n",
    "    last_filing_deadline = max(filing_deadlines[filing_deadlines['filing_deadline'] < today]['filing_deadline'])\n",
    "\n",
    "    latest_late_contribs = late_contribs[late_contribs['filing_date'] >= last_filing_deadline]\n",
    "\n",
    "    contrib_df = contribs[\n",
    "        (contribs['end_date'].isna())\n",
    "        | (contribs['receipt_date'] < contribs['end_date'])\n",
    "    ]\n",
    "    contrib_df = pd.concat([contrib_df, latest_late_contribs])[contrib_cols]\n",
    "    print(contrib_df.head(), len(contrib_df.index), sep='\\n')\n",
    "\n",
    "    output_data_dir=Path(OUTPUT_DATA_DIR)\n",
    "    output_data_dir.mkdir(exist_ok=True)\n",
    "    contribs_file_path = f'{OUTPUT_DATA_DIR}/contribs_socrata.csv'\n",
    "    save_previous_version(contribs_file_path)\n",
    "    contrib_df.to_csv(contribs_file_path, index=False)\n",
    "\n",
    "    expend_cols = (json.loads(Path(SOCRATA_EXPEND_SCHEMA_PATH).read_text(encoding='utf8'))\n",
    "    + common_cols)\n",
    "    expend_df = df[df['form'] == EXPENDITURE_FORM].rename(columns={\n",
    "        'contributor_name': 'recipient_name',\n",
    "        'contributor_address': 'recipient_address',\n",
    "        'contributor_location': 'recipient_location',\n",
    "        'receipt_date': 'expenditure_date'\n",
    "    })[expend_cols]\n",
    "    print(expend_df.head(), len(expend_df.index), sep='\\n')\n",
    "\n",
    "    expends_file_path = f'{OUTPUT_DATA_DIR}/expends_socrata.csv'\n",
    "    save_previous_version(expends_file_path)\n",
    "    expend_df.to_csv(expends_file_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--download', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    filings_json, transactions_json, filers_json = get_source_data(args.download)\n",
    "\n",
    "    main(filings_json, transactions_json, filers_json)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
